## Materials and Methods

### Honoree Curation

From [ISCB's webpage listing **ISCB Distinguished Fellows**](http://web.archive.org/web/20200116150052/https://www.iscb.org/iscb-fellows), we found recipients listed by their full names for the years 2009-2019.
We gleaned the full name of the Fellow as well as the year in which they received the honor.
To identify **ISMB Keynote Speakers**, we examined the webpage for each ISMB meeting.
We found webpages with full names for keynote speakers for the years 2002-2019.
On the PSB conference webpages, we found **PSB Keynote Speakers** for the years 1999-2020.

For the RECOMB meeting, we found conference webpages with keynote speakers for 1999, 2000, 2001, 2004, 2007, 2008, and 2010-2019.
We were able to fill in the missing years using information from the RECOMB 2016 proceedings, which summarizes the first 20 years of the RECOMB conference [@doi:10.1007/978-3-319-31957-5].
This volume has two tables of keynote speakers from 1997-2006 (Table 14, page XXVII) and 2007-2016 (Table 4, page 8).
Using these tables to verify the conference speaker lists, we arrived at two special instances of inclusion/exclusion.
Although Jun Wang was not included in these tables, we were able to confirm that he was a keynote speaker in 2011 with the RECOMB 2011 proceedings [@doi:10.1007/978-3-642-20036-6], and thus we included this speaker in the dataset.
Marian Walhout was invited as a keynote speaker but had to [cancel](http://recomb2015.mimuw.edu.pl/node/18.html) the talk due to other obligations.
Because her name was neither mentioned in the 2015 proceedings [@doi:10.1007/978-3-319-16706-0] nor in the above-mentioned tables, we excluded this speaker from our dataset.

#### Name processing

When extracting honoree names, we began with the full name as provided on the site.
Because our prediction methods required separated first and last names, we chose the first non-initial name as the first name and the final name as the last name.
We did not consider a hyphen to be a name separator:
for hyphenated names, all components were included.
For metadata from PubMed and PMC where first (fore) and last names are coded separately, we applied the same cleaning steps.
We created [functions to simplify names](https://git.dhimmel.com/pubmedpy/names.html) in the pubmedpy Python package to support standardized fore and last name processing.

### Corresponding author extraction

We assumed that, in the list of authors for a specific paper, corresponding authors (often research advisors) would be most likely to be invited for keynotes or to be honored as Fellows.
Therefore, we collected corresponding author names to assess the composition of the field, weighted by the number of corresponding authors per publication.

We evaluated two resources for extracting corresponding authors from papers: [PubMed](https://pubmed.ncbi.nlm.nih.gov/) and [PubMed Central](https://www.ncbi.nlm.nih.gov/pmc/) (PMC).
Both resources are provided by the US National Library of Medicine and index scholarly articles.
PubMed contains a record for every article published in journals it indexes (30 million records total circa 2020) and provides abstracts but not fulltext.
PMC, which provides fulltext access, does not contain every article from every journal (5.9 million records total circa 2020).
In general, open access journals will deposit their entire catalog to PMC (e.g., _BMC Bioinformatics_ & _PLOS Computational Biology_), while toll access journals (e.g., _Bioinformatics_) will only deposit articles when funders require it.
Since PMC requires publishers to submit fulltext articles in a structured XML format, the machine-readability and breadth of metadata in PMC is often superior to PubMed.

Of PMC's 5.9 million fulltext articles, only 2.7 million are part of the "[Open Access Subset](https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/)" which allows for downloading the structured fulltext as opposed to just viewing the article online.
However, authorship information does not require full text records.
We were able to download structured frontmatter (rather than fulltext) records from PMC's [OAI-PMH service](https://www.ncbi.nlm.nih.gov/pmc/tools/oai/), so we were not limited to just the Open Access Subset.
For PubMed, we used the E-Utilities APIs.
For PubMed records, we were able to extract author first and last names and their order within a record.
For PMC, we were able to extract these fields as well as whether each author was a corresponding author.
To automate and generalize these tasks, we created the [pubmedpy](https://github.com/dhimmel/pubmedpy) Python package.

We selected three journals to represent the field of bioinformatics and computational biology, including two ISCB Partner Journals (_PLOS Computational Biology_ and _Bioinformatics_) and one field-specific journal that is not a partner (_BMC Bioinformatics_).
From PubMed, we compiled a catalog of 29,755 journal articles published from when each journal was established through 2019.
We were able to retrieve authorship information for all but 6 of these articles using PubMed or PubMed Central.

To determine corresponding authors for an article, we relied on PMC data if available (20,696 articles) and otherwise fell back to PubMed data (9,053 articles).
Almost all articles without PMC data were from _Bioinformatics_ because it is a "selective deposit" rather than "full participation" journal in PMC.

We performed further analysis on PMC authors to learn more about corresponding author practices.
First, we developed and evaluated a method to infer a corresponding author when the coded corresponding status was not available.
For papers with multiple authors and at least one corresponding author, the first author was corresponding 43% of the time, whereas the last author was corresponding 62% of the time.
Therefore, we assumed the last author was corresponding when coded corresponding author status was not available (120 articles from PMC and all articles from PubMed).

Second, we investigated the number of corresponding authors for PMC articles.
81% of these articles had a single corresponding author.
1.7% had no corresponding authors.
Of these, many were editorials (e.g., [PMC1183510](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1183510/), the announcement of _PLOS Computational Biology_).
A very small number of papers had over 10 corresponding authors.
Some of these instances were true outliers, like [PMC5001208](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5001208/) with 21 corresponding authors.
Others like [PMC3509495](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3509495/) were incorrect, due to upstream errors.
To not give undue influence to papers with multiple corresponding authors, subsequent analyses on corresponding authors are inversely weighted by the number of corresponding authors per paper.

### Countries of Affiliations

Publications often provide affiliation lists for authors, which generally associate authors with research organizations and their corresponding physical addresses.
We implemented affiliation extraction in the pubmedpy Python package for both PubMed and PMC XML records.
These methods extract a sequence of textual affiliations for each author.
While ideally each affiliation record would refer to one and only one research organization, sometimes journals deposit multiple affiliations in a single structured affiliation.
For example, we extracted the following composite affiliation for all authors of [PMC4147893](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4147893/):

> 'Multimodal Computing and Interaction', Saarland University & Department for Computational Biology and Applied Computing, Max Planck Institute for Informatics, Saarbrücken, 66123 Saarland, Germany, Ray and Stephanie Lane Center for Computational Biology, Carnegie Mellon University, Pittsburgh, 15206 PA, USA, Department of Mathematics and Computer Science, Freie Universität Berlin, 14195 Berlin, Germany, Université Pierre et Marie Curie, UMR7238, CNRS-UPMC, Paris, France and CNRS, UMR7238, Laboratory of Computational and Quantitative Biology, Paris, France.

We designed a method for extracting countries from affiliations that accommodated multiple countries.
We relied on two Python utilities to extract countries from text: [geotext](https://github.com/elyase/geotext) and [`geopy.geocoders.Nominatim`](https://geopy.readthedocs.io/en/stable/#nominatim).
The first, geotext, used regular expressions to find mentions of places from the [GeoNames database](http://www.geonames.org/).
In the above text, geotext detected four mentions of places in Germany: Saarland, Saarbrücken, Saarland, Germany.
Anytime geotext identified 2 or more mentions of a country, we labeled the affiliation as including that country.

`geopy.geocoders.Nominatim` converts names / addresses to geographic coordinates using the OpenStreetMap's [Nomatim](https://nominatim.org/) service.
We split textual affiliations by punctuation and found the first segment, in reverse order, that returned any Nomatim search results.
For the above affiliation, the search order was "France", "Paris", "Laboratory of Computational and Quantitative Biology", etcetera.
Since searching "France" returns a match by Nomatim, the following queries would not be made.
When a match was found, we extracted the country containing the location.
This approach returns a single country for an affiliation when successful.
When labeling affiliations with countries, we only used these values when geotext did not return results or had ambiguity amongst countries without multiple matches.
For more details on this approach, consult the accompanying [notebook](https://github.com/greenelab/iscb-diversity/blob/5213ba3451520af3967f74d8f58553dade0a826c/07.affiliations-to-countries.ipynb) and [label dataset](https://github.com/greenelab/iscb-diversity/blob/5213ba3451520af3967f74d8f58553dade0a826c/data/affiliations/geocode.jsonl).

### Estimation of Gender

We predicted the gender of honorees and authors using the <https://genderize.io> API, which produces predictions trained on over 100 million name-gender pairings collected from the web.
We used author and honoree first names to retrieve predictions from genderize.io.
The predictions represent the probability of an honoree or author being male or female.
We used the estimated probabilities and did not convert to a hard group assignment.
For example, a query to <https://genderize.io> on January 26, 2020 for "Casey" returns a probability of male of 0.74 and a probability of female of 0.26, which we would add for an author with this first name.
Because of the limitations of considering gender as a binary trait and inferring it from first names, we only consider predictions in aggregate and not as individual values for specific scientists.

Of 411 ISCB honorees, genderize.io fails to provide gender predictions for two names.
Of 34,005 corresponding authors, 45 were missing a fore name altogether in the raw paper metadata and 1,466 had fore names consisting of only initials.
Of the remaining authors, genderize.io failed to predict gender for 1,578 of these fore names.
We note that approximately 52% of these NA predictions are hyphenated names, which is likely because they are more unique and thus are more difficult to find predictions for.
87% of these names were predicted to be of Asian origin by last name (see the race/ethnicity prediction model below).

### Estimation of Race and Ethnicity

We predicted the race and ethnicity of honorees and authors using the R package wru.
wru implements methods described in Imai and Khanna [@doi:10.1093/pan/mpw001] to predict race and ethnicity using surname and location information.
The underlying data used for prediction are derived from the US Census, in which an individual's race and ethnicity are based on their self-identification with one or more groups.
Specifically, the race categories include White, Black or African American, American Indian or Alaska Native, Asian, Native Hawaiian or Other Pacific Islander, Other race, and Two or more races [@url:https://factfinder.census.gov/help/en/race.htm], and ethnicity categories include Hispanic/Latino or Not Hispanic/Latino [@url:https://web.archive.org/web/20010405061504/http://www.census.gov/Press-Release/www/2001/raceqandas.html].
wru uses similar race/ethnicity categories but groups American Indian or Alaska Native and Native Hawaiian or Other Pacific Islander to form the Other category.

We used only the surname of author or honoree to make predictions via the *predict_race()* function from wru.
However, in the case of names that were not observed in the census, the function outputs the average demographic distribution from the census, which may produce misleading results.
To avoid this suboptimal imputation, we modified the function to return a status denoting that results were inconclusive (NA) instead.
This prediction represents the probability of an honoree or author selecting a certain race or ethnicity on a census form if they lived within the US.

Of 411 ISCB honorees, wru fails to provide race/ethnicity predictions for 98 names.
Of 34,050 corresponding authors, 40 were missing a last name in the paper metadata, and 8,770 had a last name for which wru did not provide predictions.
One limitation of wru and other methods that infer race, ethnicity, or nationality from last names is the potentially inaccurate prediction for scientists who changed their last name during marriage, a practice more common among women than men.

### Estimation of Name Origin Groups

To complement wru's race and ethnicity estimation, we developed a model to predict geographical origins of names.
The existing Python package ethnicolr [@arxiv:1805.02109] produces reasonable predictions, but its international representation in the data curated from Wikipedia in 2009 [@doi:10.1145/1557019.1557032] is still limited.
For instance, 76% of the names in ethnicolr's Wikipedia dataset are European in origin, and the dataset contains remarkably fewer Asian, African, and Middle Eastern names than wru.

To address the limitations of ethnicolr, we built a similar classifier, a Long Short-term Memory (LSTM) neural network, to infer the region of origin from patterns in the sequences of letters in full names.
We applied this model on an updated, approximately 4.5 times larger training dataset called Wiki2019 (described below).
We tested multiple character sequence lengths and, based on this comparison, selected tri-characters for the primary results described in this work.
We trained our prediction model on 80% of the Wiki2019 dataset and evaluated its performance using the remaining 20%.
This model, which we term Wiki2019-LSTM, is available in the online file [`LSTM.h5`](https://github.com/greenelab/wiki-nationality-estimate/blob/master/models/LSTM.h5).

To generate a training dataset for name origin prediction that reflects a modern naming landscape, we scraped the English Wikipedia's category of [Living People](https://en.wikipedia.org/wiki/Category:Living_people).
This category, which contained approximately 930,000 pages at the time of processing in November 2019, is regularly curated and allowed us to avoid pages related to non-persons.
For each Wikipedia page, we used two strategies to find a full birth name and location context for that person.
First, we used information from the personal details sidebar; the information in this sidebar varied widely but often contained a full name and a place of birth.
Second, in the body of the text of most English-language biographical Wikipedia pages, the first sentence usually begins with, for example, "John Edward Smith (born 1 January 1970) is an American novelist known for ..."
This structure comes from editor [guidance on biography articles](https://en.wikipedia.org/wiki/Wikipedia:Manual_of_Style/Biography#Context) and is designed to capture:

> ... the country of which the person is a citizen, national or permanent resident, or if the person is notable mainly for past events, the country where the person was a citizen, national or permanent resident when the person became notable.

We used regular expressions to parse out the person's name from this structure and checked that the expression after "is a" matched a list of nationalities.
We were able to define a name and nationality for 708,493 people by using the union of these strategies.
This process produced country labels that were more fine-grained than the broader patterns that we sought to examine among honorees and authors.
We initially grouped names by continent, but later decided to model our categorization after the hierarchical taxonomy used by [NamePrism](http://www.name-prism.com/about) [@doi:10.1145/3132847.3133008].
The NamePrism taxonomy was derived from name-country pairs by producing an embedding of names by Twitter contact patterns and then grouping countries using the similarity of names from those countries.
In an earlier version of this manuscript we also used category names derived from NamePrism, but the titles of the groupings were problematic, so we have recoded the groupings to letters.
The countries associated with each grouping are shown in Fig {@fig:nameprism_countries}.
Table @tbl:example_names shows the size of the training set for each of these groupings as well as a few examples of PubMed author names that had at least 90% prediction probability in that group.
We refer to this dataset as Wiki2019 (available online in [`annotated_names.tsv`](https://github.com/greenelab/wiki-nationality-estimate/blob/master/data/annotated_names.tsv)).

![NamePrism groups countries by name similarity. We used this grouping and
 recoded names assigned to groups in the initial publication to letter keys.
](https://raw.githubusercontent.com/greenelab/iscb-diversity/master/figs/2020-01-31_groupings.png){#fig:nameprism_countries width="70%"}

| Group | Training Size | Example Names |
| ------ | ------ | -------------------- |
| A | 280,644 | Julie S. Miller, Jesse A. Livezey, Jeremy C Simpson, Chris Smith, Thomas M Drudge |
| B | 188,918 | Sven Poths, Céline Feillet, Frederik Otzen Bagger, Lars I. Leichert, Sebastian MB Nijman |
| C | 54,197 | Jee-Hyub Kim, Yoriko Takahashi, Xiaohua Xu, Xuehai Zhang, Yoshihiro Noguchi |
| D | 66,391 | Beatriz Peñalver Bernabé, Diego Miranda-Saavedra, Marcelo Lobosco, Euler Guimarães Horta, Edgar E Vallejo-Clemente |
| E | 20,025 | Mahender Kumar Singh, Vidhu Choudhary, Suraj Pradhan, Ramakant Sharma, Vinod Menon |
| F | 30,703 | Mohammad R. K. Mofrad, Fikret Ercal, Mehdi Yousfi Monod, Ghazaleh Taherzadeh, Noora Al Muftah |
| G | 4,549 | Tal Vider-Shalit, Itsik Pe'er, Michal Lavidor, Yoav Gothilf, Dvir Netanely |
| H | 16,105 | Samuel A Assefa, Nyaradzo M. Mgodi, Stanley Kimbung Mbandi, Oyebode J Oyeyemi, Ezekiel Adebiyi |

Table: **Predicting name-origin groups of names trained on Wikipedia's living people.**
The table lists the 8 groups and the number of living people for each region that the LSTM was trained on.
Example names shows actual author names that received a high prediction for each region.
Full information about which countries comprised each region can be found in the online dataset [`country_to_region.tsv`](https://github.com/greenelab/iscb-diversity/blob/make-letters/data/countries/2020-01-31_groupings.tsv).
{#tbl:example_names}

### Affiliation Analysis

Along with the corresponding author names, we collected their affiliations recorded in each publication for this analysis.
During the honoree curation process, if an honoree was listed with their affiliation at the time, we recorded this affiliation for analysis.
For ISCB Fellows, we used the affiliation listed on the ISCB page.
Because we could not find affiliations for the 1997 and 1998 RECOMB keynote speakers' listed for these years, they were left blank.
If an author or speaker has more than one affiliation, each is inversely weighted by the number of affiliations that individual has.
